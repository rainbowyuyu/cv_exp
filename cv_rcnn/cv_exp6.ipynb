{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T04:37:51.631764Z",
     "start_time": "2024-12-13T04:37:51.618659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import cv2"
   ],
   "id": "522ce10fbec5c60a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T04:37:14.605169Z",
     "start_time": "2024-12-13T04:37:14.591427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取 JSON 文件中的标注信息\n",
    "def read_annotation(annotation_path):\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        annotation = json.load(f)\n",
    "    return annotation\n",
    "\n",
    "# 从 JSON 中提取目标信息\n",
    "def extract_objects(annotations):\n",
    "    objects = []\n",
    "    for obj in annotations.get(\"objects\", []):\n",
    "        class_title = obj.get(\"classTitle\")\n",
    "        points = obj.get(\"points\", {}).get(\"exterior\", [])\n",
    "\n",
    "        if class_title and points:\n",
    "            # 通过外部坐标生成边界框\n",
    "            xmin = min([p[0] for p in points])\n",
    "            xmax = max([p[0] for p in points])\n",
    "            ymin = min([p[1] for p in points])\n",
    "            ymax = max([p[1] for p in points])\n",
    "\n",
    "            objects.append({\n",
    "                \"class\": class_title,\n",
    "                \"bbox\": [xmin, ymin, xmax, ymax]\n",
    "            })\n",
    "    return objects\n",
    "\n",
    "# 预处理图像：读取、调整大小、归一化\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "# 生成数据集\n",
    "def create_dataset(image_folder, annotation_folder):\n",
    "    # 获取图像文件夹中所有图像文件路径\n",
    "    image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    dataset = []\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        # 获取对应的标注文件路径\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        json_path = os.path.join(annotation_folder, image_name + '.jpg.json')  # 正确的标注文件路径\n",
    "\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"警告：找不到标注文件：{json_path}\")\n",
    "            continue\n",
    "\n",
    "        # 读取标注数据\n",
    "        annotations = read_annotation(json_path)\n",
    "        objects = extract_objects(annotations)\n",
    "\n",
    "        # 预处理图像\n",
    "        image = preprocess_image(image_path)\n",
    "\n",
    "        # 获取目标边界框和类别标签\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for obj in objects:\n",
    "            bboxes.append(obj[\"bbox\"])\n",
    "            labels.append(obj[\"class\"])\n",
    "\n",
    "        # 将图像、边界框和标签组合成数据\n",
    "        dataset.append({\n",
    "            \"image\": image,\n",
    "            \"bboxes\": bboxes,\n",
    "            \"labels\": labels\n",
    "        })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_unique_classes(dataset):\n",
    "    all_classes = set()\n",
    "    for data in dataset:\n",
    "        for label in data['labels']:\n",
    "            all_classes.add(label)\n",
    "    return list(all_classes)"
   ],
   "id": "f7b7c1533a577705",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T04:37:40.928746Z",
     "start_time": "2024-12-13T04:37:15.706985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 示例使用\n",
    "train_dir = r\"laboro-tomato-DatasetNinja\\Train\"\n",
    "test_dir = r\"laboro-tomato-DatasetNinja\\Test\"\n",
    "\n",
    "train_annotation_path = os.path.join(train_dir, \"ann\")\n",
    "train_images_path = os.path.join(train_dir, \"img\")\n",
    "\n",
    "val_annotation_path = os.path.join(test_dir, \"ann\")\n",
    "val_images_path = os.path.join(test_dir, \"img\")\n",
    "\n",
    "# 创建训练和验证数据集\n",
    "train_dataset = create_dataset(train_images_path, train_annotation_path)\n",
    "val_dataset = create_dataset(val_images_path, val_annotation_path)\n",
    "\n",
    "# 所有类别\n",
    "train_classes = get_unique_classes(train_dataset)\n",
    "num_classes = len(train_classes)\n",
    "\n",
    "print(f\"训练集类别数量：{num_classes}\")\n",
    "print(f\"所有类别：{train_classes}\")\n",
    "\n",
    "print(f\"训练集包含 {len(train_dataset)} 个样本\")\n",
    "print(f\"第1个样本的图像形状：{train_dataset[0]['image'].shape}\")\n",
    "print(f\"第1个样本的边界框数量：{len(train_dataset[0]['bboxes'])}\")\n",
    "print(f\"第1个样本的标签数量：{len(train_dataset[0]['labels'])}\")\n",
    "\n",
    "print(f\"验证集包含 {len(val_dataset)} 个样本\")\n",
    "print(f\"第1个样本的图像形状：{val_dataset[0]['image'].shape}\")\n",
    "print(f\"第1个样本的边界框数量：{len(val_dataset[0]['bboxes'])}\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集类别数量：6\n",
      "所有类别：['b_half_ripened', 'b_green', 'b_fully_ripened', 'l_green', 'l_half_ripened', 'l_fully_ripened']\n",
      "训练集包含 153 个样本\n",
      "第1个样本的图像形状：(4032, 3024, 3)\n",
      "第1个样本的边界框数量：6\n",
      "第1个样本的标签数量：6\n",
      "验证集包含 59 个样本\n",
      "第1个样本的图像形状：(4032, 3024, 3)\n",
      "第1个样本的边界框数量：9\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# yolo",
   "id": "e1dabceff6947a6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T04:37:47.695204Z",
     "start_time": "2024-12-13T04:37:40.960009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 获取类别 ID\n",
    "def get_class_id(class_name, class_list):\n",
    "    if class_name not in class_list:\n",
    "        class_list.append(class_name)\n",
    "    return class_list.index(class_name)\n",
    "\n",
    "# 将目标转换为 YOLO 格式\n",
    "def convert_to_yolo_format(image, bboxes, labels, class_list):\n",
    "    img_height, img_width, _ = image.shape\n",
    "    yolo_bboxes = []\n",
    "\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        # 计算边界框的中心坐标和宽高，并进行归一化\n",
    "        x_center = round((xmin + xmax) / 2.0 / img_width,6)\n",
    "        y_center = round((ymin + ymax) / 2.0 / img_height,6)\n",
    "        width = round((xmax - xmin) / img_width,6)\n",
    "        height = round((ymax - ymin) / img_height,6)\n",
    "\n",
    "        # 获取类别 ID\n",
    "        class_id = get_class_id(label, class_list)\n",
    "\n",
    "        # 格式化为 YOLO 格式\n",
    "        yolo_bboxes.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "\n",
    "    return yolo_bboxes\n",
    "\n",
    "# 生成数据集并保存为 YOLO 标注文件\n",
    "def create_yolo_dataset(image_folder, annotation_folder, output_folder):\n",
    "    # 获取图像文件夹中所有图像文件路径\n",
    "    image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    class_list = []  # 用于存储所有类别\n",
    "    for image_path in image_paths:\n",
    "        # 获取对应的标注文件路径\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        json_path = os.path.join(annotation_folder, image_name + '.jpg.json')  # 正确的标注文件路径\n",
    "\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"警告：找不到标注文件：{json_path}\")\n",
    "            continue\n",
    "\n",
    "        # 读取标注数据\n",
    "        annotations = read_annotation(json_path)\n",
    "        objects = extract_objects(annotations)\n",
    "\n",
    "        # 预处理图像\n",
    "        image = preprocess_image(image_path)\n",
    "\n",
    "        # 获取目标边界框和类别标签\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for obj in objects:\n",
    "            bboxes.append(obj[\"bbox\"])\n",
    "            labels.append(obj[\"class\"])\n",
    "\n",
    "        # 转换为 YOLO 格式\n",
    "        yolo_bboxes = convert_to_yolo_format(image, bboxes, labels, class_list)\n",
    "\n",
    "        # 保存 YOLO 格式的标注到文件\n",
    "        yolo_txt_path = os.path.join(output_folder, image_name + '.txt')\n",
    "        with open(yolo_txt_path, 'w') as f:\n",
    "            for yolo_bbox in yolo_bboxes:\n",
    "                f.write(yolo_bbox + '\\n')\n",
    "\n",
    "    # 返回所有类别的列表\n",
    "    return class_list\n",
    "\n",
    "# 使用示例\n",
    "image_folder = r'E:\\ipynb\\project\\cv_exp_rcnn\\laboro-tomato-DatasetNinja\\Test\\img'\n",
    "annotation_folder = r'E:\\ipynb\\project\\cv_exp_rcnn\\laboro-tomato-DatasetNinja\\Test\\ann'\n",
    "output_folder = r'E:\\ipynb\\project\\cv_exp_rcnn\\laboro-tomato-DatasetNinja\\Test\\img'\n",
    "\n",
    "create_yolo_dataset(image_folder, annotation_folder, output_folder)"
   ],
   "id": "b35b747693e0a5bf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b_green',\n",
       " 'b_half_ripened',\n",
       " 'b_fully_ripened',\n",
       " 'l_green',\n",
       " 'l_fully_ripened',\n",
       " 'l_half_ripened']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# voc",
   "id": "eb2661c5ba7ed395"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T04:45:36.149691Z",
     "start_time": "2024-12-13T04:45:28.957131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from xml.etree.ElementTree import Element, SubElement, ElementTree\n",
    "\n",
    "# 创建 Pascal VOC 格式的 XML 文件\n",
    "def create_voc_xml(image_path, bboxes, labels, output_folder):\n",
    "    image_name = os.path.basename(image_path)\n",
    "    img = cv2.imread(image_path)\n",
    "    img_height, img_width, _ = img.shape\n",
    "    \n",
    "    # 创建 XML 根元素\n",
    "    annotation = Element('annotation')\n",
    "    \n",
    "    # 添加文件夹、文件名和图像路径\n",
    "    folder = SubElement(annotation, 'folder')\n",
    "    folder.text = 'images'\n",
    "    \n",
    "    filename = SubElement(annotation, 'filename')\n",
    "    filename.text = image_name\n",
    "    \n",
    "    path = SubElement(annotation, 'path')\n",
    "    path.text = image_path\n",
    "    \n",
    "    # 添加图像尺寸信息\n",
    "    size = SubElement(annotation, 'size')\n",
    "    width = SubElement(size, 'width')\n",
    "    width.text = str(img_width)\n",
    "    height = SubElement(size, 'height')\n",
    "    height.text = str(img_height)\n",
    "    depth = SubElement(size, 'depth')\n",
    "    depth.text = '3'  # 3 表示 RGB 图像\n",
    "    \n",
    "    # 为每个目标添加标注\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        obj = SubElement(annotation, 'object')\n",
    "        \n",
    "        name = SubElement(obj, 'name')\n",
    "        name.text = label\n",
    "        \n",
    "        bndbox = SubElement(obj, 'bndbox')\n",
    "        xmin = SubElement(bndbox, 'xmin')\n",
    "        xmin.text = str(bbox[0])\n",
    "        ymin = SubElement(bndbox, 'ymin')\n",
    "        ymin.text = str(bbox[1])\n",
    "        xmax = SubElement(bndbox, 'xmax')\n",
    "        xmax.text = str(bbox[2])\n",
    "        ymax = SubElement(bndbox, 'ymax')\n",
    "        ymax.text = str(bbox[3])\n",
    "    \n",
    "    # 生成 XML 文件并保存\n",
    "    tree = ElementTree(annotation)\n",
    "    output_path = os.path.join(output_folder, os.path.splitext(image_name)[0] + '.xml')\n",
    "    tree.write(output_path)\n",
    "\n",
    "# 生成数据集并保存为 Pascal VOC 标注文件\n",
    "def create_voc_dataset(image_folder, annotation_folder, output_folder):\n",
    "    # 获取图像文件夹中所有图像文件路径\n",
    "    image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        # 获取对应的标注文件路径\n",
    "        image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        json_path = os.path.join(annotation_folder, image_name + '.jpg.json')  # 正确的标注文件路径\n",
    "\n",
    "        if not os.path.exists(json_path):\n",
    "            print(f\"警告：找不到标注文件：{json_path}\")\n",
    "            continue\n",
    "\n",
    "        # 读取标注数据\n",
    "        annotations = read_annotation(json_path)\n",
    "        objects = extract_objects(annotations)\n",
    "\n",
    "        # 获取目标边界框和类别标签\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for obj in objects:\n",
    "            bboxes.append(obj[\"bbox\"])\n",
    "            labels.append(obj[\"class\"])\n",
    "\n",
    "        # 创建 Pascal VOC 格式的 XML 文件\n",
    "        create_voc_xml(image_path, bboxes, labels, output_folder)\n",
    "\n",
    "# 使用示例\n",
    "image_folder = r'E:\\ipynb\\project\\cv_exp_rcnn\\laboro-tomato-DatasetNinja\\Test\\img'\n",
    "annotation_folder = r'E:\\ipynb\\project\\cv_exp_rcnn\\laboro-tomato-DatasetNinja\\Test\\ann'\n",
    "output_folder = r'E:\\ipynb\\project\\cv_exp_rcnn\\laboro-tomato-DatasetNinja\\Test\\voc'\n",
    "\n",
    "create_voc_dataset(image_folder, annotation_folder, output_folder)\n"
   ],
   "id": "685aeb67cc138ed5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:00:28.042432Z",
     "start_time": "2024-12-13T05:00:28.028741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import xml.etree.ElementTree as ET"
   ],
   "id": "cd307eca691776f6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:00:28.513567Z",
     "start_time": "2024-12-13T05:00:28.500376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self, image_folder, annotation_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.annotation_folder = annotation_folder\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取图像路径\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_folder, image_name)\n",
    "        annotation_path = os.path.join(self.annotation_folder, os.path.splitext(image_name)[0] + '.xml')\n",
    "\n",
    "        # 读取图像\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # 解析标注文件\n",
    "        boxes, labels = self.parse_annotation(annotation_path)\n",
    "\n",
    "        # 如果有需要，应用转换\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 目标格式化为字典：boxes 是边界框，labels 是类别标签\n",
    "        target = {\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "        return image, target\n",
    "\n",
    "    def parse_annotation(self, annotation_path):\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in root.findall('object'):\n",
    "            # 获取边界框坐标\n",
    "            xmin = int(obj.find('bndbox').find('xmin').text)\n",
    "            ymin = int(obj.find('bndbox').find('ymin').text)\n",
    "            xmax = int(obj.find('bndbox').find('xmax').text)\n",
    "            ymax = int(obj.find('bndbox').find('ymax').text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            # 获取类别标签\n",
    "            labels.append(self.get_class_id(obj.find('name').text))\n",
    "\n",
    "        return boxes, labels\n",
    "\n",
    "    def get_class_id(self, class_name):\n",
    "        # 你可以根据实际数据设置不同的类别\n",
    "        class_dict = r\"E:\\ipynb\\project\\cv_exp_rcnn\\laboro-tomato-DatasetNinja\\tomato\\class.txt\"\n",
    "        return class_dict.get(class_name, 0)  # 默认类别 0\n",
    "\n"
   ],
   "id": "410222449de4b861",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:00:29.324982Z",
     "start_time": "2024-12-13T05:00:29.315252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((800, 800)),  # 可根据需要调整大小\n",
    "])\n",
    "\n",
    "# 创建数据集实例\n",
    "train_dataset = VOCDataset(image_folder=r\"E:\\ipynb\\project\\cv_exp_rcnn\\laboro-tomato-DatasetNinja\\Train\\img\", \n",
    "                           annotation_folder=r\"E:\\ipynb\\project\\cv_exp_rcnn\\laboro-tomato-DatasetNinja\\Train\\voc\", \n",
    "                           transform=transform)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
   ],
   "id": "7322aaa807f05af5",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:00:30.373502Z",
     "start_time": "2024-12-13T05:00:29.948661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision.models.detection as detection\n",
    "import torch.optim as optim\n",
    "\n",
    "# 加载预训练的 Faster R-CNN 模型\n",
    "model = detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# 将模型设置为训练模式\n",
    "model.train()\n",
    "\n",
    "# 修改类别数量，通常默认是 91 类，我们假设只有 4 个类别（背景+3个目标类别）\n",
    "num_classes = 5  # 4 个目标类别 + 1 背景\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# 定义优化器\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# 设置学习率调度器\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n"
   ],
   "id": "f5bab3365ba63a78",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ipynb\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\ipynb\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:00:32.091075Z",
     "start_time": "2024-12-13T05:00:31.132664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 训练模式\n",
    "    running_loss = 0.0\n",
    "    for images, targets in train_loader:\n",
    "        # 通过 GPU 训练（如果有的话）\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # 前向传播\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        # 损失值\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += losses.item()\n",
    "\n",
    "    # 更新学习率\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # 输出每个 epoch 的损失\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n"
   ],
   "id": "264887e11ce8344c",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [8, 4] at entry 0 and [20, 4] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()  \u001B[38;5;66;03m# 训练模式\u001B[39;00m\n\u001B[0;32m      5\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, targets \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;66;03m# 通过 GPU 训练（如果有的话）\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     images \u001B[38;5;241m=\u001B[39m [image\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[0;32m      9\u001B[0m     targets \u001B[38;5;241m=\u001B[39m [{k: v\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitems()} \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m targets]\n",
      "File \u001B[1;32mE:\\ipynb\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[0;32m    707\u001B[0m ):\n",
      "File \u001B[1;32mE:\\ipynb\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    759\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mE:\\ipynb\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ipynb\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    338\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[0;32m    340\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    397\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 398\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\ipynb\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    208\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m    212\u001B[0m         collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[0;32m    214\u001B[0m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\ipynb\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    208\u001B[0m transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mbatch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m--> 212\u001B[0m         \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    213\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[0;32m    214\u001B[0m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\ipynb\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMutableMapping):\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001B[39;00m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001B[39;00m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001B[39;00m\n\u001B[0;32m    169\u001B[0m     clone \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mcopy(elem)\n\u001B[0;32m    170\u001B[0m     clone\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[1;32m--> 171\u001B[0m         {\n\u001B[0;32m    172\u001B[0m             key: collate(\n\u001B[0;32m    173\u001B[0m                 [d[key] \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map\n\u001B[0;32m    174\u001B[0m             )\n\u001B[0;32m    175\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem\n\u001B[0;32m    176\u001B[0m         }\n\u001B[0;32m    177\u001B[0m     )\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m clone\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\ipynb\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collections\u001B[38;5;241m.\u001B[39mabc\u001B[38;5;241m.\u001B[39mMutableMapping):\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001B[39;00m\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001B[39;00m\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001B[39;00m\n\u001B[0;32m    169\u001B[0m     clone \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mcopy(elem)\n\u001B[0;32m    170\u001B[0m     clone\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[0;32m    171\u001B[0m         {\n\u001B[1;32m--> 172\u001B[0m             key: \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[43md\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\n\u001B[0;32m    174\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    175\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m elem\n\u001B[0;32m    176\u001B[0m         }\n\u001B[0;32m    177\u001B[0m     )\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m clone\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mE:\\ipynb\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 155\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32mE:\\ipynb\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001B[0m, in \u001B[0;36mcollate_tensor_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    270\u001B[0m     storage \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_new_shared(numel, device\u001B[38;5;241m=\u001B[39melem\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    271\u001B[0m     out \u001B[38;5;241m=\u001B[39m elem\u001B[38;5;241m.\u001B[39mnew(storage)\u001B[38;5;241m.\u001B[39mresize_(\u001B[38;5;28mlen\u001B[39m(batch), \u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(elem\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m--> 272\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: stack expects each tensor to be equal size, but got [8, 4] at entry 0 and [20, 4] at entry 1"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T05:06:16.581247Z",
     "start_time": "2024-12-13T05:06:16.254383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"fasterrcnn.pth\")\n",
    "\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load(\"fasterrcnn.pth\"))\n",
    "model.eval()\n"
   ],
   "id": "c7c5ce1ee2aef7ea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rainbow_yu\\AppData\\Local\\Temp\\ipykernel_24900\\1292110035.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"fasterrcnn.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=20, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "42eaae5a113fe5ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
